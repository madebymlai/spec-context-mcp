# Embeddings (required for semantic search)
EMBEDDING_PROVIDER=voyageai
EMBEDDING_API_KEY=
EMBEDDING_MODEL=
EMBEDDING_BASE_URL=
EMBEDDING_RERANK_MODEL=
EMBEDDING_RERANK_URL=
EMBEDDING_RERANK_FORMAT=auto
EMBEDDING_RERANK_BATCH_SIZE=
# EMBEDDING_DIMENSION is currently ignored (model defines dimensions)
EMBEDDING_DIMENSION=

# VoyageAI alias (optional alternative to EMBEDDING_API_KEY for voyageai provider)
VOYAGEAI_API_KEY=

# OpenAI API key for LLM reasoning (not used for embeddings)
OPENAI_API_KEY=

# Optional
# Python executable for ChunkHound.
# Leave unset to let spec-context choose a local `.venv/bin/python` if present.
# Set this only if you've installed the Python deps into that interpreter/venv.
# CHUNKHOUND_PYTHON=python3
DASHBOARD_URL=http://localhost:3000

# OpenRouter API key (used for dashboard AI review and ChunkHound LLM)
OPENROUTER_API_KEY=

# Disable dashboard npm version check (optional)
SPEC_CONTEXT_DISABLE_VERSION_CHECK=false

# ChunkHound LLM for deep research
# Options: openai, ollama, claude-code-cli, codex-cli, gemini, anthropic, opencode-cli
CHUNKHOUND_LLM_PROVIDER=claude-code-cli
CHUNKHOUND_LLM_BASE_URL=
CHUNKHOUND_LLM_UTILITY_MODEL=
CHUNKHOUND_LLM_SYNTHESIS_MODEL=

# ChunkHound runtime tuning (optional)
CHUNKHOUND_EMBED_SWEEP_SECONDS=300
CHUNKHOUND_EMBED_SWEEP_BACKOFF_SECONDS=30
CHUNKHOUND_FILE_QUEUE_MAXSIZE=2000
CHUNKHOUND_FILE_QUEUE_DRAIN_SECONDS=1.0

# Discipline mode: full (TDD+reviews), standard (reviews), minimal (verification only)
SPEC_CONTEXT_DISCIPLINE=full

# CLI dispatch for multi-LLM orchestration
# Set to agent name (claude, codex, gemini, opencode) â€” flags are resolved automatically.
# Or set a custom command (passed through as-is).
# SPEC_CONTEXT_IMPLEMENTER=claude
# SPEC_CONTEXT_REVIEWER=codex

# Optional model tiering by task complexity (handled internally by dispatch-runtime).
# Runtime classifies each task as simple|complex, then builds dispatch_cli with model flags.
# Works with known providers above; custom commands ignore these vars.
#
# Generic model tier vars:
# SPEC_CONTEXT_IMPLEMENTER_MODEL_SIMPLE=
# SPEC_CONTEXT_IMPLEMENTER_MODEL_COMPLEX=
# SPEC_CONTEXT_REVIEWER_MODEL_SIMPLE=
# SPEC_CONTEXT_REVIEWER_MODEL_COMPLEX=
#
# Codex-only optional reasoning tier vars:
# SPEC_CONTEXT_IMPLEMENTER_REASONING_EFFORT_SIMPLE=medium
# SPEC_CONTEXT_IMPLEMENTER_REASONING_EFFORT_COMPLEX=xhigh
# SPEC_CONTEXT_REVIEWER_REASONING_EFFORT_SIMPLE=medium
# SPEC_CONTEXT_REVIEWER_REASONING_EFFORT_COMPLEX=xhigh
#
# Example: Claude implementer + Codex reviewer
# SPEC_CONTEXT_IMPLEMENTER=claude
# SPEC_CONTEXT_IMPLEMENTER_MODEL_SIMPLE=sonnet-4.5
# SPEC_CONTEXT_IMPLEMENTER_MODEL_COMPLEX=opus-4.6
# SPEC_CONTEXT_REVIEWER=codex
# SPEC_CONTEXT_REVIEWER_MODEL_SIMPLE=codex-5.3
# SPEC_CONTEXT_REVIEWER_MODEL_COMPLEX=codex-5.3
# SPEC_CONTEXT_REVIEWER_REASONING_EFFORT_SIMPLE=medium
# SPEC_CONTEXT_REVIEWER_REASONING_EFFORT_COMPLEX=xhigh
#
# Example: Gemini implementer + OpenCode reviewer
# SPEC_CONTEXT_IMPLEMENTER=gemini
# SPEC_CONTEXT_IMPLEMENTER_MODEL_SIMPLE=gemini-2.5-flash
# SPEC_CONTEXT_IMPLEMENTER_MODEL_COMPLEX=gemini-2.5-pro
# SPEC_CONTEXT_REVIEWER=opencode
# SPEC_CONTEXT_REVIEWER_MODEL_SIMPLE=openai/gpt-5-mini
# SPEC_CONTEXT_REVIEWER_MODEL_COMPLEX=anthropic/claude-sonnet-4-5
